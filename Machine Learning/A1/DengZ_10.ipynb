{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DengZ_10.ipynb","provenance":[{"file_id":"1e7Bmd2ACvB7TDUyu7SWVivbOatmJbICI","timestamp":1622751942516},{"file_id":"1IgrabRnbdaOqPQhhMsVRPChxMTQ0BujH","timestamp":1622684953159},{"file_id":"1boMa2tAqkd6AEgT2F7JG04KaStASqCFk","timestamp":1622680091737},{"file_id":"1ZGrr6c8KnW1LsIcMnJaFMTGu4NA1pw6Z","timestamp":1622584681158}],"collapsed_sections":[],"authorship_tag":"ABX9TyP9yAtNdRdc268a2FkwV1Dt"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3wbWs2-fCfZR"},"source":["## Student: Deng, Zixuan (V00971633)\n","# Problem 10"]},{"cell_type":"markdown","metadata":{"id":"Bu1eS8qUCkL6"},"source":["## Algebraic regularization [20 points]\n","\n","come up with the closed-form formula for **w** when using ridge regression that penalizes the sum of squared coefficients. What would that be? Provide the formal proof.   "]},{"cell_type":"markdown","metadata":{"id":"xOi9BIyej-uP"},"source":["We know that Ridge( L2) regression is: \n","$$ f(w) = \\sum_{i} (y_i - w \\cdot x_i)^2 + \\lambda \\sum_{j} w_j^2  $$, when write in matrix form it becomes following: \n","$$ (Y-W^TX)^T(Y-W^TX) + \\lambda W^TW      $$\n","\n","\n","The object is to solve the following problem:\n","$$ \\min_W (Y -W^TX)^T(Y-W^TX) + \\lambda W^TW    $$\n","\n","Letting first derivative of equation equals to 0, we will find the solution. \n","$$\\frac{\\partial (Y -W^TX)^T(Y-W^TX)}{\\partial W} = -2X^T(Y-W^TX)$$, \n","$$\\frac{\\partial \\lambda W^TW}{\\partial W} = 2 \\lambda W$$\n","Let first derivative equals to 0: \n","$$   -2X^T(Y-W^TX) + 2 \\lambda W = 0  $$\n","\n","$$ X^TY = X^TXW + \\lambda W $$\n","$$  X^TY = (X^TX + \\lambda)W $$\n","$$  W = (X^TX + \\lambda I)^{-1}X^TY       $$"]}]}